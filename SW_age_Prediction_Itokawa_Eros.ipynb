{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Surface age prediction using saved models\n",
        "\n",
        "This code predict the SW age at 1 AU using the already saved models.\n",
        "\n",
        "For itokawa, models saved in \"Itokawa_Models\" (These models were saved using the \"Itokawa_model_training.ipynb\")\n",
        "\n",
        "For Eros, models saved in \"Eros_Models\" (These models were saved using the \"Eros_model_training.ipynb\").\n",
        "\n",
        "In \"Surface age correction\", assign \"a\" 1.3 for Itokawa and 1.4 for Eros (Semi-major axis of Itokawa and Eros)."
      ],
      "metadata": {
        "id": "erMEEoYi0Gs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.17.1 keras==3.5.0"
      ],
      "metadata": {
        "id": "Q-EuSdVTKBps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras==0.13.0"
      ],
      "metadata": {
        "id": "Zi-DPWexKB1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "id": "4PtC-DYwKB_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 joblib==1.4.2"
      ],
      "metadata": {
        "id": "SfejmTmcKN8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sklearn\n",
        "import joblib\n",
        "import os\n",
        "import math\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from keras.layers import Dense, Conv1D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import VotingRegressor,GradientBoostingRegressor,RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import RegressorMixin, BaseEstimator\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vM4uAb0rKQVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN model\n",
        "def create_cnn_model_1(input_shape,learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=32, kernel_size=2, activation=\"relu\", padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation=\"relu\"))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[\"mean_absolute_error\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "jM95NsOHKUMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Keras Regressor class for CNN-based regression tasks.\n",
        "\n",
        "class CNNRegressor(KerasRegressor):\n",
        "    _estimator_type = \"regressor\"\n",
        "\n",
        "    def __init__(self, input_shape, learning_rate=0.001, epochs=10, batch_size=32, verbose=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_shape = input_shape\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        #super().__init__(build_fn=self._keras_build_fn, **kwargs)\n",
        "\n",
        "    def _keras_build_fn(self):\n",
        "        return create_cnn_model_1(self.input_shape, self.learning_rate)"
      ],
      "metadata": {
        "id": "VS-FRqpiKXja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define choices for H ion irradiation (H) and lasser irradiation (L)\n",
        "choices = {'H': 3, 'L': 12}\n",
        "\n",
        "input_file = \"/file_with_the_reflectance.xlsx\"  # Replace with your file name\n",
        "input_data = pd.read_excel(input_file)\n",
        "\n",
        "# Retain the first two columns as identifiers\n",
        "identifiers = input_data.iloc[:, :3]  # First two columns\n",
        "corrected = identifiers.copy()\n",
        "\n",
        "# Separate features (X) and target (y), excluding the first two columns\n",
        "X = input_data.iloc[:, 5:-1].to_numpy()  # Columns 3 to second-last as features\n",
        "y = input_data.iloc[:, -1].copy()       # The last column (target)\n",
        "\n",
        "# Initialize a DataFrame to store predictions for both H and L\n",
        "all_predictions_combined = identifiers.copy()  # Start with identifiers\n",
        "\n",
        "# Folder containing the saved models\n",
        "models_folder = \"/trained_models\" # Replace with your folder name\n",
        "\n",
        "for choice, adjustment_value in choices.items():\n",
        "    # Adjust the target values based on the current choice\n",
        "    y_adjusted = y + adjustment_value\n",
        "\n",
        "    # Reconstruct the modified dataset for predictions\n",
        "    X_modified = np.hstack([X, y_adjusted.values.reshape(-1, 1)])\n",
        "\n",
        "    # Initialize a DataFrame to store predictions for this choice\n",
        "    predictions = []\n",
        "    for iteration in range(1, 31):\n",
        "        model_filename = os.path.join(models_folder, f\"En_Eros_{iteration}.joblib\")\n",
        "        print(f\"Loading model: {model_filename} for choice {choice}\")\n",
        "        ensemble_model = joblib.load(model_filename)\n",
        "\n",
        "        # Ensure input data is in the correct format\n",
        "        X_modified = X_modified.astype(np.float32)\n",
        "\n",
        "        # Predict using the loaded model\n",
        "        y_pred = ensemble_model.predict(X_modified)\n",
        "\n",
        "        # Scale predictions\n",
        "        y_pred_scaled = 10**y_pred - 1\n",
        "\n",
        "        predictions.append(y_pred_scaled)\n",
        "\n",
        "        # Add predictions for this iteration to the DataFrame\n",
        "        all_predictions_combined[f\"{iteration}_{choice}\"] = y_pred_scaled\n",
        "\n",
        "    # Convert predictions to a numpy array for statistical calculations\n",
        "    predictions_array = np.array(predictions)  # Shape: (30, num_samples)\n",
        "\n",
        "    # Calculate mean and standard deviation for each sample\n",
        "    mean_predictions = predictions_array.mean(axis=0)  # Mean along iterations\n",
        "    std_predictions = predictions_array.std(axis=0)    # Std deviation along iterations\n",
        "\n",
        "    # Add mean and std to the combined DataFrame\n",
        "    all_predictions_combined[f\"Mean_{choice}\"] = mean_predictions\n",
        "    all_predictions_combined[f\"Std_{choice}\"] = std_predictions\n",
        "\n",
        "    corrected[f\"Mean_{choice}\"] = mean_predictions\n",
        "    corrected[f\"Std_{choice}\"] = std_predictions\n",
        "\n",
        "# Save all predictions and statistics to a single Excel file\n",
        "output_file = \"/predictions.xlsx\"\n",
        "all_predictions_combined.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Predictions and statistics saved to {output_file}\")"
      ],
      "metadata": {
        "id": "5_fgAhXSKaga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqWy-epzKanu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surface age correction\n"
      ],
      "metadata": {
        "id": "o9rkZKJkxUo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#surface age corrected (H ion) = surface age at 1 AU * (distance)^2\n",
        "\n",
        "#For Itokawa  a = 1.3 for Eros a = 1.4\n",
        "a=1.3\n",
        "\n",
        "H_corrected = merged_df['Mean_H']*a**2\n",
        "H_std_corrected = merged_df['Std_H']*a**2\n",
        "\n",
        "merged_df['H_corrected'] = H_corrected\n",
        "merged_df['H_std_corrected'] = H_std_corrected"
      ],
      "metadata": {
        "id": "dmLckXF6NIk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(H_corrected)"
      ],
      "metadata": {
        "id": "AkHnkZrvjE4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base on Grun et. al (1991) and Divine et. al (1993) and Jehn et. al (200)\n",
        "\n",
        "W = 10**(-12)   #weight of the particle in g\n",
        "B = (math.log10(W)+11.5)/5.5\n",
        "B2 = 1-B**2\n",
        "gamma = (math.log10(W)+12)/6\n",
        "F = 10**(-4)    # flux at 1 AU\n",
        "T = 365*24*3600 #time in s\n",
        "A1AU = 0.00035478\n",
        "\n",
        "o6 = (0.138+0.142 * am+ 0.408*a**2)** - 1\n",
        "o12 =(6.8-1.96 * a + 0.16 * (a**2))\n",
        "frel_1 = (5**B2)*o6\n",
        "frel_2 = gamma**2+(1-gamma**2)*o12\n",
        "frel = frel_1/frel_2\n",
        "f = F*frel  #fluence\n",
        "v = 11300*a**2 - 38900*a + 42600  #velocity m/s interpotalet between values at 1 Au, 2 Au and 3 AU.\n",
        "\n",
        "A= 0.5*W/1000*(v**2)*f*T  #1/2*particle weight*Impact average velocity*Flux*Time per year in seconds\n",
        "\n",
        "#surface age corrected (laser) = surface age at 1 AU * A1AU/A\n",
        "\n",
        "L_corrected = all_predictions_combined[\"Mean_L\"]*(A1AU/A)\n",
        "L_std_corrected =all_predictions_combined['Std_L']*(A1AU/A)\n",
        "\n",
        "all_predictions_combined['L_corrected'] = L_corrected\n",
        "all_predictions_combined['L_std_corrected'] = L_std_corrected\n",
        "\n",
        "corrected['L_corrected'] = L_corrected\n",
        "corrected['L_std_corrected'] = L_std_corrected\n",
        "\n",
        "\n",
        "# Save to an Excel file\n",
        "merged_df.to_excel('/Surface_age_IEros.xlsx', index=False) # replace with your file name"
      ],
      "metadata": {
        "id": "mohTGxfsjHtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yv4kX0Z1j2mC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}