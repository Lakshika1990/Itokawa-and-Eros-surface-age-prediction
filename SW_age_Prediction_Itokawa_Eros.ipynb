{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Surface age prediction using saved models\n",
        "\n",
        "This code predict the SW age at 1 AU using the already saved models.\n",
        "\n",
        "For itokawa, models saved in \"Itokawa_Models\" (These models were saved using the \"Itokawa_model_training.ipynb\")\n",
        "\n",
        "For Eros, models saved in \"Eros_Models\" (These models were saved using the \"Eros_model_training.ipynb\").\n",
        "\n",
        "In \"Surface age correction\", assign \"a\" 1.3 for Itokawa and 1.4 for Eros (Semi-major axis of Itokawa and Eros)."
      ],
      "metadata": {
        "id": "erMEEoYi0Gs5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf4NXToZftg7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sklearn\n",
        "import joblib\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ml_dtypes>=0.5"
      ],
      "metadata": {
        "id": "-1CpqDSronE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U jax jaxlib ml_dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv-WXEM3hTWd",
        "outputId": "618e7148-80ff-43a4-abc5-b5f7e4ded0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jax\n",
            "  Downloading jax-0.9.0.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.9.0.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.3)\n",
            "Downloading jax-0.9.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.9.0.1-cp312-cp312-manylinux_2_27_x86_64.whl (80.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "Successfully installed jax-0.9.0.1 jaxlib-0.9.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zapHuyPcpWiZ",
        "outputId": "d6faaf55-9e50-4217-ae1b-5292010e1c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (26.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import interp1d\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from keras.layers import Dense, Conv1D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import VotingRegressor,GradientBoostingRegressor,RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import RegressorMixin, BaseEstimator\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "NIL6kSWffuoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN model\n",
        "def create_cnn_model_1(input_shape,learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=16, kernel_size=2, activation=\"relu\", padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation=\"relu\"))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[\"mean_absolute_error\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "7P6i4VUSpsTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Keras Regressor class for CNN-based regression tasks.\n",
        "\n",
        "class CNNRegressor(KerasRegressor):\n",
        "    _estimator_type = \"regressor\"\n",
        "\n",
        "    def __init__(self, input_shape, learning_rate=0.001, epochs=10, batch_size=32, verbose=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_shape = input_shape\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        #super().__init__(build_fn=self._keras_build_fn, **kwargs)\n",
        "\n",
        "    def _keras_build_fn(self):\n",
        "        return create_cnn_model_1(self.input_shape, self.learning_rate)"
      ],
      "metadata": {
        "id": "RuwjAHbYpxFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define choices for H ion irradiation (H) and lasser irradiation (L)\n",
        "#1-olivine with H+, 2-pyroxene with H+, 3- OC/mixtures with H+\n",
        "# 10- olivine with laser, 11- Pyroxene with laser, 12- OC/mixture with laser\n",
        "\n",
        "choices = {'H': 2, 'L': 11}\n",
        "\n",
        "# Load the input data sheet\n",
        "input_file = \"/Itokawa_model.xlsx\"  # Replace with your file name\n",
        "input_data = pd.read_excel(input_file)\n",
        "\n",
        "# Retain the first two columns as identifiers\n",
        "identifiers = input_data.iloc[:, :3]  # First two columns\n",
        "corrected = identifiers.copy()\n",
        "\n",
        "# Separate features (X) and target (y), excluding the first two columns\n",
        "X = input_data.iloc[:, 5:-1].to_numpy()  # Columns 3 to second-last as features\n",
        "y = input_data.iloc[:, -1].copy()       # The last column (target)\n",
        "\n",
        "# Initialize a DataFrame to store predictions for both H and L\n",
        "all_predictions_combined = identifiers.copy()  # Start with identifiers\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(input_data.columns[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBRg6U-P1Knc",
        "outputId": "2d2abded-7643-4bb9-e54b-c096e59e1012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (928, 9)\n",
            "Index(['550nm', '594nm', '638nm', '682nm', '726nm', '770nm', '814nm', '858nm',\n",
            "       '902nm'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = \"Predictions\"\n",
        "\n",
        "for choice, adjustment_value in choices.items():\n",
        "\n",
        "  y_adjusted  = y + adjustment_value\n",
        "\n",
        "  X_modified = np.hstack([X, y_adjusted.values.reshape(-1, 1)])\n",
        "\n",
        "print(X_modified.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1oQdVsZ1K1m",
        "outputId": "f3033d64-d3ab-44a4-a3d6-6d61f1f034bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(928, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths and parameters ---\n",
        "model_folder = \"/Itokawa_Models\"\n",
        "n_iterations = 30\n",
        "\n",
        "# Ensemble weights (same as before)\n",
        "weights = np.array([4, 4, 1, 5, 1])\n",
        "weights = weights / weights.sum()\n",
        "\n",
        "# Placeholder for all iterations' predictions\n",
        "all_preds = []\n",
        "\n",
        "for choice, adjustment_value in choices.items():\n",
        "  # Adjust the target values based on the current choice\n",
        "  y_adjusted  = y + adjustment_value\n",
        "\n",
        "   # Reconstruct the modified dataset for predictions\n",
        "  X_modified = np.hstack([X, y_adjusted.values.reshape(-1, 1)])\n",
        "\n",
        "  for iteration in range(1, 31):\n",
        "      print(f\"Loading ensemble {iteration}\")\n",
        "\n",
        "      # Load models\n",
        "      cnn_path = os.path.join(model_folder, f\"Model_{iteration}_cnn.keras\")\n",
        "      gbr_path = os.path.join(model_folder, f\"Model_{iteration}_gbr.joblib\")\n",
        "      knn_path = os.path.join(model_folder, f\"Model_{iteration}_knn.joblib\")\n",
        "      etr_path = os.path.join(model_folder, f\"Model_{iteration}_etr.joblib\")\n",
        "      rft_path = os.path.join(model_folder, f\"Model_{iteration}_rft.joblib\")\n",
        "\n",
        "      for path in [cnn_path, gbr_path, knn_path, etr_path, rft_path]:\n",
        "          if not os.path.exists(path):\n",
        "              print(f\"Missing file for iteration {iteration}: {os.path.basename(path)}\")\n",
        "              continue\n",
        "\n",
        "      # Load models\n",
        "      cnn_reg = load_model(cnn_path)\n",
        "      gbr = joblib.load(gbr_path)\n",
        "      knn = joblib.load(knn_path)\n",
        "      etr = joblib.load(etr_path)\n",
        "      rft = joblib.load(rft_path)\n",
        "\n",
        "      preds = [\n",
        "          cnn_reg.predict(X_modified),\n",
        "          gbr.predict(X_modified),\n",
        "          knn.predict(X_modified),\n",
        "          etr.predict(X_modified),\n",
        "          rft.predict(X_modified)\n",
        "      ]\n",
        "\n",
        "      # Weighted average\n",
        "      preds = [np.ravel(p) for p in preds]\n",
        "      weighted_pred = np.average(preds, axis=0, weights=weights)\n",
        "\n",
        "      y_pred_scaled = 10**weighted_pred - 1\n",
        "\n",
        "      all_preds.append(y_pred_scaled)\n",
        "      all_predictions_combined[f\"{iteration}_{choice}\"] = y_pred_scaled\n",
        "\n",
        "  predictions_array = np.array(all_preds)\n",
        "\n",
        "  mean_predictions = predictions_array.mean(axis=0)\n",
        "  std_predictions = predictions_array.std(axis=0)\n",
        "\n",
        "  all_predictions_combined[f\"Mean_{choice}\"] = mean_predictions\n",
        "  all_predictions_combined[f\"Std_{choice}\"] = std_predictions\n",
        "\n",
        "  corrected[f\"Mean_{choice}\"] = mean_predictions\n",
        "  corrected[f\"Std_{choice}\"] = std_predictions\n",
        "\n",
        "# Save all predictions and statistics to a single Excel file\n",
        "output_file = \"/Predictions/Itokawa_results_H_and_L.xlsx\"\n",
        "all_predictions_combined.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Predictions and statistics for both 'H' and 'L' saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_NTq_iMpQV",
        "outputId": "bd4d6ec2-906d-40b4-8bdf-d593ddbb9ef0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ensemble 1\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 2\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 3\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 4\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 6\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 7\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 8\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 9\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 11\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 12\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 13\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 14\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 15\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 16\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 17\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 18\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 19\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
            "Loading ensemble 21\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 22\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 23\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 24\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 25\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 26\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 27\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 28\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 29\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 30\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 1\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 2\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 3\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 4\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 6\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 7\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 8\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 9\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 11\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 12\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 13\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 14\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 15\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 16\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Loading ensemble 17\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 18\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
            "Loading ensemble 19\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 21\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 22\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 23\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Loading ensemble 24\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 25\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 26\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 27\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 28\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 29\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Loading ensemble 30\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
            "Predictions and statistics for both 'H' and 'L' saved to /content/drive/My Drive/Part3/Predictions/Vesta_Gaia2.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surface age correction\n"
      ],
      "metadata": {
        "id": "o9rkZKJkxUo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#surface age corrected (H ion) = surface age at 1 AU * (distance)^2\n",
        "\n",
        "#For Itokawa  a = 1.3 for Eros a = 1.4\n",
        "a=1.3\n",
        "\n",
        "H_corrected = merged_df['Mean_H']*a**2\n",
        "H_std_corrected = merged_df['Std_H']*a**2\n",
        "\n",
        "merged_df['H_corrected'] = H_corrected\n",
        "merged_df['H_std_corrected'] = H_std_corrected"
      ],
      "metadata": {
        "id": "dmLckXF6NIk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(H_corrected)"
      ],
      "metadata": {
        "id": "AkHnkZrvjE4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base on Grun et. al (1991) and Divine et. al (1993) and Jehn et. al (200)\n",
        "\n",
        "W = 10**(-12)   #weight of the particle in g\n",
        "B = (math.log10(W)+11.5)/5.5\n",
        "B2 = 1-B**2\n",
        "gamma = (math.log10(W)+12)/6\n",
        "F = 10**(-4)    # flux at 1 AU\n",
        "T = 365*24*3600 #time in s\n",
        "A1AU = 0.00035478\n",
        "\n",
        "o6 = (0.138+0.142 * am+ 0.408*a**2)** - 1\n",
        "o12 =(6.8-1.96 * a + 0.16 * (a**2))\n",
        "frel_1 = (5**B2)*o6\n",
        "frel_2 = gamma**2+(1-gamma**2)*o12\n",
        "frel = frel_1/frel_2\n",
        "f = F*frel  #fluence\n",
        "v = 11300*a**2 - 38900*a + 42600  #velocity m/s interpotalet between values at 1 Au, 2 Au and 3 AU.\n",
        "\n",
        "A= 0.5*W/1000*(v**2)*f*T  #1/2*particle weight*Impact average velocity*Flux*Time per year in seconds\n",
        "\n",
        "#surface age corrected (laser) = surface age at 1 AU * A1AU/A\n",
        "\n",
        "L_corrected = all_predictions_combined[\"Mean_L\"]*(A1AU/A)\n",
        "L_std_corrected =all_predictions_combined['Std_L']*(A1AU/A)\n",
        "\n",
        "all_predictions_combined['L_corrected'] = L_corrected\n",
        "all_predictions_combined['L_std_corrected'] = L_std_corrected\n",
        "\n",
        "corrected['L_corrected'] = L_corrected\n",
        "corrected['L_std_corrected'] = L_std_corrected\n",
        "\n",
        "\n",
        "# Save to an Excel file\n",
        "merged_df.to_excel('/Surface_age_Itokawa.xlsx', index=False)"
      ],
      "metadata": {
        "id": "mohTGxfsjHtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yv4kX0Z1j2mC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}